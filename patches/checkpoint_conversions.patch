Submodule Megatron-LM contains modified content
diff --git a/Megatron-LM/megatron/training/arguments.py b/Megatron-LM/megatron/training/arguments.py
index d8d08f61..ced6968e 100644
--- a/Megatron-LM/megatron/training/arguments.py
+++ b/Megatron-LM/megatron/training/arguments.py
@@ -124,7 +124,6 @@ def load_retro_args(args):
     # Return if no project directory is specified.
     if args.retro_project_dir is None:
         return
-
     # Load retro config.
     retro_config = load_retro_config(args.retro_project_dir)
 
@@ -706,7 +705,7 @@ def validate_args(args, defaults={}):
     if args.num_experts is not None:
         assert args.spec is None, "Model Spec must be None when using MoEs"
     
-    if args.tensor_model_parallel_size > 1:
+    if args.tensor_model_parallel_size > 1 and args.num_experts:
             assert args.sequence_parallel, \
                 "When using MoE and tensor parallelism, sequence parallelism must be used."
                 
@@ -934,8 +933,6 @@ def _add_retro_args(parser):
 
     group.add_argument('--retro-project-dir', default=None,
                        help='Retro project directory, which contains the '
-                       'preprocessed data for pretraining. This directory '
-                       'is built during preprocessing (see '
                        'tools/retro/README.md), and contains subdirectories '
                        'for the chunk database and pretraining neighbors.')
     group.add_argument('--retro-add-retriever',
diff --git a/Megatron-LM/megatron/training/checkpointing.py b/Megatron-LM/megatron/training/checkpointing.py
index 92813050..ba7578a6 100644
--- a/Megatron-LM/megatron/training/checkpointing.py
+++ b/Megatron-LM/megatron/training/checkpointing.py
@@ -1016,7 +1016,7 @@ def load_args_from_checkpoint(
             _set_arg('virtual_pipeline_model_parallel_size', force=True)
             _set_arg('num_layers_per_virtual_pipeline_stage')
             _set_arg('expert_model_parallel_size', force=True)
-
+            
     return args, checkpoint_args
 
 
diff --git a/Megatron-LM/tools/checkpoint/loader_mcore.py b/Megatron-LM/tools/checkpoint/loader_mcore.py
index 9185969b..86948641 100644
--- a/Megatron-LM/tools/checkpoint/loader_mcore.py
+++ b/Megatron-LM/tools/checkpoint/loader_mcore.py
@@ -93,6 +93,7 @@ def _load_checkpoint(queue, args):
     margs.use_legacy_models = False
     margs.transformer_impl = args.loader_transformer_impl
 
+    margs.tensor_model_parallel_size = checkpoint_args.tensor_model_parallel_size
     def check_for_arg(arg_name, default=None):
         if getattr(margs, arg_name, None) is None:
             if default is not None:
diff --git a/Megatron-LM/tools/checkpoint/saver_mcore.py b/Megatron-LM/tools/checkpoint/saver_mcore.py
index ef8a1d61..62b5ecc8 100644
--- a/Megatron-LM/tools/checkpoint/saver_mcore.py
+++ b/Megatron-LM/tools/checkpoint/saver_mcore.py
@@ -186,7 +186,7 @@ def save_checkpoint(queue, args):
     # Explicitly copy sequence_parallel, apply_query_key_layer_scaling.
     margs.sequence_parallel = md.checkpoint_args.sequence_parallel
     margs.apply_query_key_layer_scaling = md.checkpoint_args.apply_query_key_layer_scaling
-
+    
     # Sequence parallel is required if use both tensor-parallel and Moe.
     if margs.num_experts is not None and args.target_tensor_parallel_size is not None:
         if margs.num_experts > 1 and args.target_tensor_parallel_size > 1:
diff --git a/gpu-interactive.sh b/gpu-interactive.sh
old mode 100644
new mode 100755
index 6392ac1..b15b39f
--- a/gpu-interactive.sh
+++ b/gpu-interactive.sh
@@ -7,6 +7,6 @@ srun \
     --ntasks-per-node=1 \
     --gres=gpu:mi250:1 \
     --time=01:00:00 \
-    --mem=100G\
+    --mem=0\
     --pty \
     bash
